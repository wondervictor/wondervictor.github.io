<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Real-Time Open-Vocabulary Detection">
  <meta name="keywords" content="multi-modal object detetion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>YOLO-World</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./images/smalllogo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img width=500px src="./images/banner.png">
            <br><br>
            <!-- <h1 class="title is-1 publication-title">YOLO-WorldüëÄ</h1> -->
            <h1 class="title is-2 publication-title">Real-Time Open-Vocabulary Object Detection</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="" style="color:#008AD7;font-weight:normal;">Tianheng Cheng<sup>2,3,*</sup></a>&nbsp,
              </span>
              <span class="author-block">
                <a href="https://linsong.info" style="color:#008AD7;font-weight:normal;">Lin
                  Song<sup>1,*,&#128231;</sup></a>&nbsp,
              </span>
              <span class="author-block">
                <a href="https://geyixiao.com" style="color:#008AD7;font-weight:normal;">Yixiao Ge<sup>1,2,‚≠ê</sup></a>&nbsp,
              </span>
              <span class="author-block">
                <a href="https://eic.hust.edu.cn/professor/liuwenyu" style="color:#008AD7;font-weight:normal;">Wenyu
                  Liu<sup>3</sup></a>&nbsp,
              </span>
              <span class="author-block">
                <a href="https://xwcv.github.io/" style="color:#008AD7;font-weight:normal;">Xinggang
                  Wang<sup>3,&#128231;</sup></a>&nbsp,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=zh-CN"
                  style="color:#008AD7;font-weight:normal;">Ying Shan<sup>1,2</sup></a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#008AD7; font-weight:normal"><sup>1</sup></b> Tencent AI
                Lab</span>,
              <span class="author-block"><b style="color:#008AD7; font-weight:normal"><sup>2</sup></b> ARC Lab, Tencent
                PCG</span>,
              <span class="author-block"><b style="color:#008AD7; font-weight:normal"><sup>3</sup></b> Huazhong
                University of Science and Technology</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup><b>Equal Contribution</b></span>&nbsp;&nbsp
              <span class="author-block"><sup>&#128231</sup><b> Corresponding Author</b></span>&nbsp;&nbsp
              <span class="author-block"><sup>‚≠ê</sup><b> Project Lead</b></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.17270" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/AILAB-CVC/YOLO-World" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/stevengrove/YOLO-World" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-share-square"></i>
                    </span>
                    <span>HuggingFace</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <font size=4>
          <table width="77%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>üî•<b><span style="color: #ff3860">What's New</span></b></heading>
                  <div style="height: 150px; overflow: auto;">
                    <ul>
                      <li>
                        [2024.1.31] The <a href="https://arxiv.org/abs/2401.17270"><b>technical report</b></a> of
                          <b>YOLO-World</b> are available now!
                          
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </font>
      </div>
    </div>
  </section>


  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">üìñ Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical
              tools.
              However, their reliance on predefined and trained object categories limits their applicability in open
              scenarios.
              Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with
              open-vocabulary
              detection capabilities through vision-language modeling and pre-training on large-scale datasets.
              Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and
              region-text
              contrastive loss to facilitate the interaction between visual and linguistic information.
              Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency.
              On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100,
              which outperforms many state-of-the-art methods in terms of both accuracy and speed.
              Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks,
              including object detection and open-vocabulary instance segmentation.

          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="section">
    <div class="col                                                                                                                                                     umns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><img src="./images/smalllogo.png" width="60px"> YOLO-World</h2>
        <div class="container is-max-desktop content">
          <h4 class="title is-5" align="left">üåü Highlights</h4>
          <table align="center" border="0" cellspacing="0" cellpadding="20" class="has-text-justified"
            style="margin-top:-30px;">
            <tbody>
              <tr>
                <td>
                  <div>
                    <ul>
                      <li> YOLO-World is the <b>next-generation of YOLO detectors</b>, aiming for real-time open-vocabulary object
                        detection.</li>
                      <li> YOLO-World is pre-trained on large-scale vision-language datasets, including Objects365, GQA,
                        Flickr30K, and CC3M, which <b>enpowers YOLO-World with strong zero-shot open-vocabulary capbility
                          and grounding ability</b> in images. </li>
                      <li> YOLO-World achieves <b>fast inference speeds</b> and we present re-parameterization
                        techniques for faster inference and deployment given users' vocabularies.</li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <h4 class="title is-5" align="left">‚öôÔ∏è <b><span>Framework</span></b></h4>

          <table align="center" border="0" cellspacing="0" cellpadding="20" class="has-text-justified"
            style="margin-top:-30px;">
            <tbody>
              <tr>
                <td>
                  <div>
                    <ul>
                      <li>
                        The YOLO-World builds the YOLO detector with the <b>frozen CLIP-based text encoder</b> for
                        extracting text embeddings from the input texts, e.g., object categories or noun phrases.
                      </li>
                      <li>
                        The YOLO-World contains an <b>Re-parameterizable Vision-Language Path Aggregation Network
                          (RepVL-PAN)</b> to facilitate the interaction between multi-scale image features and text
                        embeddings.
                        The RepVL-PAN can re-parameterize the user's offline vocabularies into the model parameters for
                        fast inference and deployment.
                      </li>
                      <li>
                        The YOLO-World is pre-trained on <b>large-scale region-text datasets</b> with the <b>region-text
                          contrastive loss</b> to learn the region-level alignment between vision and language.
                        For normal image-text datasets, e.g., CC3M, we adopt an automatic labeling approach to generate
                        pseudo region-text pairs.
                      </li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <div style="text-align: center;">
            <img id="teaser" width="80%" src="images/yolo_arch.png">
          </div><br>
          <div style="text-align: left;">
            Please check more details in our <a href="">technical report</a>.
          </div>
        </div>


      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->
  </section>



  <section class="section" id="Performance">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">üìä Performance</h2>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h4 class="title is-5">
            <font color="#008AD7">1. Zero-Shot Evaluation on LVIS</font>
          </h4>
          <p>
            We compare the zero-shot performance on LVIS (minival) of recent open-vocabulary detectors:
            <style>
              table.GeneratedTable {
                width: 100%;
                background-color: #ffffff;
                border-collapse: collapse;
                border-width: 2px;
                border-color: #c1c4c5;
                border-style: solid;
                color: #000000;
              }
              
              table.GeneratedTable td, table.GeneratedTable th {
                border-width: 2px;
                border-color: #9b9d9e;
                border-style: solid;
                padding: 3px;
              }
              
              table.GeneratedTable thead {
                background-color: #49AAA7;
              }
            </style>
            <div class="column is-six-fifths" width="80%" align="center">
            <table class="GeneratedTable">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Backbone</th>
                  <th>Pre-trained Data</th>
                  <th>FPS(V100)</th>
                  <th>AP</th>
                  <th>AP<sub>r</sub></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>GLIP-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG</td>
                  <td>0.12</td>
                  <td>24.9</td>
                  <td>17.7</td>
                </tr>
                <tr>
                  <td>GLIP-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG,Cap4M</td>
                  <td>0.12</td>
                  <td>26.0</td>
                  <td>20.8</td>
                </tr>
                <tr>
                  <td>GLIPv2-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG</td>
                  <td>0.12</td>
                  <td>26.9</td>
                  <td>-</td>
                </tr>
                <tr>
                  <td>GLIPv2-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG,Cap4M</td>
                  <td>0.12</td>
                  <td>29.0</td>
                  <td>-</td>
                </tr>
                <tr>
                  <td>GroundingDINO-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG</td>
                  <td>1.5</td>
                  <td>25.6</td>
                  <td>14.4</td>
                </tr>
                <tr>
                  <td>GroundingDINO-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG,Cap4M</td>
                  <td>1.5</td>
                  <td>27.4</td>
                  <td>18.1</td>
                </tr>
                <tr>
                  <td>DetCLIP-T</td>
                  <td>Swin-T</td>
                  <td>O365,GoldG</td>
                  <td>2.3</td>
                  <td>34.4</td>
                  <td>26.9</td>
                </tr>
                <tr bgcolor="#87C669">
                  <td>YOLO-World-S</td>
                  <td>YOLOv8-S</td>
                  <td>O365,GoldG</td>
                  <td>74.1</td>
                  <td>26.2</td>
                  <td>19.1</td>
                </tr>
                <tr bgcolor="#87C669">
                  <td>YOLO-World-M</td>
                  <td>YOLOv8-M</td>
                  <td>O365,GoldG</td>
                  <td>58.1</td>
                  <td>31.0</td>
                  <td>23.8</td>
                </tr>

                <tr bgcolor="#87C669">
                  <td>YOLO-World-L</td>
                  <td>YOLOv8-L</td>
                  <td>O365,GoldG</td>
                  <td>52.0</td>
                  <td>35.0</td>
                  <td>27.1</td>
                </tr>
                <tr bgcolor="#87C669">
                  <td>YOLO-World-L</td>
                  <td>YOLOv8-L</td>
                  <td>O365,GoldG,CC-250K</td>
                  <td>52.0</td>
                  <td>35.4</td>
                  <td>27.6</td>
                </tr>
              </tbody>
            </table>
            Zero-shot Evaluation on LVIS minival
          </div>
            
          </p>
          <h4 class="title is-5">
            <font color="#008AD7">2. Speed and Accuracy Curve</font>
          </h4>
          <p>
            We compare the speed and accuracy curve of pre-trained YOLO-World vesus recent open-vocabulary detectors on zero-shot LVIS evaluation:
            <div align="center"><img src="./images/speed_acc.png" width="50%"></div>
          </p>
          <h4 class="title is-5">
            <font color="#008AD7">3. Visualizations</font>
          </h4>
          <p>
            We provide some visualization results generated by the pre-trained YOLO-World(L):
            <div align="center"><img src="./images/vis_lvis.png" width="80%"><br><span>(a) Visualization Results on Zero-shot Inference on LVIS</span></div>
            <div align="center"><img src="./images/user_vocab.png" width="80%"><br><span>(b) Visualization Results on User‚Äôs Vocabulary</span></div>
            <div align="center"><img src="./images/vis_referring.png" width="80%"><br><span>(c) Visualization Results on Referring Object Detection</span></div>

          </p>
          
        </div>
      </div>
    </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <b>If you find YOLO-World is useful in your research or applications, please consider giving us a citation.</b>
      <br> <br>

      <pre><code>
        @article{cheng2024yolow,
          title={YOLO-World: Real-Time Open-Vocabulary Object Detection},
          author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
          journal={arXiv preprint arXiv:},
          year={2024}
        }
  </code></pre>
    </div>
  </section>

  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a
          href="https://llava-vl.github.io/">LLaVA</a>, and <a href="https://sharegpt4v.github.io/">ShareGPT4V</a>, licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </section>

</body>

</html>